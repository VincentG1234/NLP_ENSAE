{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install pandas openai transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "ms_marco             20000\n",
       "natural_questions    19999\n",
       "rag_12000             9590\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"raw_queries.csv\")\n",
    "df.head()\n",
    "df[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialise le client Groq (à faire une seule fois)\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "# Prompt template avec {query}\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Your task is to take a raw search query and rewrite it to improve retrieval performance in a search engine or RAG system.\n",
    "\n",
    "Follow these guidelines:\n",
    "- Correct any grammar or spelling errors.\n",
    "- Rephrase the query naturally and clearly.\n",
    "-  Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
    "- Keep it concise and relevant.\n",
    "\n",
    "DO NOT SAY ANYTHING ELSE THAN THE QUERY\n",
    "\n",
    "### Example\n",
    "Input: danger screen sleep \n",
    "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
    "\n",
    "Now rewrite this query:\n",
    "Input: {query}\n",
    "\"\"\"\n",
    "\n",
    "def rewrite_query_llm(query: str, temperature: float = 0.5) -> str:\n",
    "    prompt = PROMPT_TEMPLATE.format(query=query)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=128\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR] {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1. Correction — modèle bien configuré\n",
    "corrector = pipeline(\"text2text-generation\", model=\"vennify/t5-base-grammar-correction\")\n",
    "\n",
    "# 2. Reformulation\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"Vamsi/T5_Paraphrase_Paws\")\n",
    "\n",
    "# 3. Expansion via WordNet sur les mots clés\n",
    "def synonymize_key_terms(text, max_synonyms=2):\n",
    "    doc = nlp(text)\n",
    "    final_tokens = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"NOUN\", \"VERB\", \"ADJ\"} and token.is_alpha and len(token.text) > 3:\n",
    "            syns = wordnet.synsets(token.text)\n",
    "            lemmas = list(set(\n",
    "                lemma.name().replace('_', ' ') \n",
    "                for s in syns for lemma in s.lemmas() \n",
    "                if lemma.name().lower() != token.text.lower()\n",
    "            ))\n",
    "            if lemmas:\n",
    "                final_tokens.append(f\"{token.text} ({', '.join(lemmas[:max_synonyms])})\")\n",
    "            else:\n",
    "                final_tokens.append(token.text)\n",
    "        else:\n",
    "            final_tokens.append(token.text)\n",
    "    return \" \".join(final_tokens)\n",
    "\n",
    "# Pipeline complet\n",
    "def rewrite_pipeline(query, verbose=False):\n",
    "    corrected = corrector(query, max_length=64, do_sample=False)[0][\"generated_text\"]\n",
    "    explicit = paraphraser(corrected, max_length=64, do_sample=False)[0][\"generated_text\"]\n",
    "    enriched = synonymize_key_terms(explicit)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nRaw        : {query}\")\n",
    "        print(f\"Corrected  : {corrected}\")\n",
    "        print(f\"Explicit   : {explicit}\")\n",
    "        print(f\"Enriched   : {enriched}\")\n",
    "    return enriched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Chargement du modèle Flan-T5\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "FLAN_PROMPT_TEMPLATE = \"\"\"\n",
    "Rewrite the following query to improve information retrieval.\n",
    "- Correct grammar or spelling\n",
    "- Rephrase it naturally and clearly\n",
    "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
    "- Keep it short and useful\n",
    "\n",
    "### Example\n",
    "Input: danger screen sleep\n",
    "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
    "\n",
    "Now rewrite this query:\n",
    "Input: {query}\n",
    "\"\"\"\n",
    "\n",
    "# Création du pipeline\n",
    "flan_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Fonction de rewriting\n",
    "def rewrite_with_flan(query: str, temperature=0.5, max_tokens=100) -> str:\n",
    "    prompt = FLAN_PROMPT_TEMPLATE.format(query=query)\n",
    "    result = flan_pipe(\n",
    "        prompt,\n",
    "        max_length=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature\n",
    "    )[0][\"generated_text\"]\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests comparatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(30, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what do chromosomes look like in bacteria'\n",
      " 'what are chromosomes kid definition'\n",
      " 'who coined the term aerobics and in what year'\n",
      " \"What is the proposed change to the parental leave payments scheme by the Coalition's small business spokesman, Mr Billson?\"\n",
      " \"What is Sean Kane's educational background?\"\n",
      " \"What is the concern of the Savannah Tree Foundation regarding the new law school's parking lot?\"\n",
      " 'name the animal from which we get wool' 'what is keg coupler'\n",
      " 'What are the functions of the KitchenAid Mixer attachment pack?'\n",
      " 'where is uc santa barbara located'\n",
      " 'what is mitochondrial disease symptoms' 'liable definition law'\n",
      " 'what language do people from iceland speak'\n",
      " 'What is the author struggling with?'\n",
      " 'wii fit u vs wii fit plus difference'\n",
      " 'who sang if you like pina coladas lyrics'\n",
      " 'who are the freedom riders and what are they trying to accomplish'\n",
      " 'best method to cook a sirloin tip steak'\n",
      " 'What is the legal status of prostitution in Hamilton, Canada?'\n",
      " 'What is the purpose of the report compiled by the Pakistan Federal Union of Journalists (PFUJ) in 2015?'\n",
      " \"are mcdonald's chicken nuggets made with real chicken\"\n",
      " 'when is peter pan introduced in once upon a time'\n",
      " 'what are rare eye colors' 'what is the concept of no child left behind'\n",
      " 'when do we vote for mid term elections'\n",
      " 'why are there robot animals in horizon zero dawn'\n",
      " 'Who visited the ravaged homes in East Haven after Tropical Storm Irene and promised federal help to cover some of the costs?'\n",
      " 'Who was the host of the syndicated radio program \"On the Radio\" from 1985 until 1992?'\n",
      " 'autoimmune diseases that affect the lungs'\n",
      " 'who tried to kill julie in desperate housewives']\n"
     ]
    }
   ],
   "source": [
    "sample_queries_list = sample['query'].values\n",
    "print(sample_queries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahya name meaning\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: yahya name meaning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: The meaning of the name yahya is:\n",
      "--------------------------------------------------\n",
      "where does the electron transport chain occur in prokaryotes and eukaryotes\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: where does the electron transport chain occur in prokaryotes and eukaryotes\n",
      "\n",
      "Where does the electron transport chain occur in prokaryotes and eukaryotes?\n",
      "--------------------------------------------------\n",
      "what happened to the house in amityville horror\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: what happened to the house in amityville horror\n",
      "\n",
      "What happened to the house in amityville horror?\n",
      "--------------------------------------------------\n",
      "Who is Cung Le hoping to have a rematch with?\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: Who is Cung Le hoping to have a rematch with?\n",
      "\n",
      "Cung Le is hoping to have a rematch with whom?\n",
      "--------------------------------------------------\n",
      "What are the benefits of eating low-glycemic foods according to the context?\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: What are the benefits of eating low-glycemic foods according to the context?\n",
      "\n",
      "Low-glycemic foods have low levels of triglycerides, which means they are not good for your health.\n",
      "--------------------------------------------------\n",
      "what to avoid during early pregnancy\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: what to avoid during early pregnancy\n",
      "\n",
      "What are some things to avoid during early pregnancy?\n",
      "--------------------------------------------------\n",
      "seattle delta airline lost and found phone number\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: seattle delta airline lost and found phone number\n",
      "\n",
      "seattle delta airline lost and found phone number\n",
      "--------------------------------------------------\n",
      "who played wheeler on law and order criminal intent\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: who played wheeler on law and order criminal intent\n",
      "\n",
      "Wheeler played on law and order criminal intent.\n",
      "--------------------------------------------------\n",
      "What is the purpose of the Des Moines Latino Film Festival?\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: What is the purpose of the Des Moines Latino Film Festival?\n",
      "\n",
      "What is the purpose of the Des Moines Latino Film Festival?\n",
      "--------------------------------------------------\n",
      "Who is in the band 'Pearl Jam'?\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: Who is in the band 'Pearl Jam'?\n",
      "\n",
      "Who is in the band Pearl Jam?\n",
      "--------------------------------------------------\n",
      "pnb share price\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: pnb share price\n",
      "\n",
      "The pnb share price is 0.09%.\n",
      "--------------------------------------------------\n",
      "how to clean grates on weber gas grill\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: how to clean grates on weber gas grill\n",
      "\n",
      "How do you clean grates on a weber gas grill?\n",
      "--------------------------------------------------\n",
      "after how many weeks we will able to find baby gender in pregnancy\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: after how many weeks we will able to find baby gender in pregnancy\n",
      "\n",
      "After how many weeks we will able to find baby gender in pregnancy?\n",
      "--------------------------------------------------\n",
      "when was star spangled banner put to music\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: when was star spangled banner put to music\n",
      "\n",
      "Star Spangled Banner was put to music in the 1960s.\n",
      "--------------------------------------------------\n",
      "when was the song i'll fly away written\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: when was the song i'll fly away written\n",
      "\n",
      "When was the song i'll fly away written?\n",
      "--------------------------------------------------\n",
      "what is lean production in business\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: what is lean production in business\n",
      "\n",
      "What is lean production in business?\n",
      "--------------------------------------------------\n",
      "what kind of fault was mount pinatubo\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: what kind of fault was mount pinatubo\n",
      "\n",
      "What kind of fault was mount pinatubo?\n",
      "--------------------------------------------------\n",
      "what does a dna sequencer do\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: what does a dna sequencer do\n",
      "\n",
      "What does a dna sequencer do?\n",
      "--------------------------------------------------\n",
      "what is friction tape\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: what is friction tape\n",
      "\n",
      "What is friction tape?\n",
      "--------------------------------------------------\n",
      "What would a child of Thor and Wonder Woman be like?\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: What would a child of Thor and Wonder Woman be like?\n",
      "\n",
      "What would a child of Thor and Wonder Woman be like?\n",
      "--------------------------------------------------\n",
      "how much meat per person for a party brisket\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: how much meat per person for a party brisket\n",
      "\n",
      "How much meat do you need per person for a party brisket?\n",
      "--------------------------------------------------\n",
      "why weighted bell boots\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: why weighted bell boots\n",
      "\n",
      "Why weighted bell boots?\n",
      "--------------------------------------------------\n",
      "who wrote the songs for la la land\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: who wrote the songs for la la land\n",
      "\n",
      "Who wrote the songs for La La Land?\n",
      "--------------------------------------------------\n",
      "how long to cook cut up potatoes in the oven\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: how long to cook cut up potatoes in the oven\n",
      "\n",
      "How long should it take to cook cut up potatoes in the oven?\n",
      "--------------------------------------------------\n",
      "operating cost to heat a pool heat pump\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: operating cost to heat a pool heat pump\n",
      "\n",
      "The operating cost of a pool heat pump is less than the operating cost of a hot water heater.\n",
      "--------------------------------------------------\n",
      "where will the world cup be played in 2022\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: where will the world cup be played in 2022\n",
      "\n",
      "The 2022 world cup will be played in Canada.\n",
      "--------------------------------------------------\n",
      "muscle relaxers used for sleep\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: muscle relaxers used for sleep\n",
      "\n",
      "What is the use of muscle relaxers for sleep?\n",
      "--------------------------------------------------\n",
      "when was the first internal combustion engine created\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: when was the first internal combustion engine created\n",
      "\n",
      "The first internal combustion engine was created in 1903.\n",
      "--------------------------------------------------\n",
      "Who commuted John White's prison sentence?\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: Who commuted John White's prison sentence?\n",
      "\n",
      "John White was sentenced to a minimum of 11 years in prison.\n",
      "--------------------------------------------------\n",
      "how do you address a university chancellor\n",
      "\n",
      "Rewrite the following query to improve information retrieval.\n",
      "- Correct grammar or spelling\n",
      "- Rephrase it naturally and clearly\n",
      "- Add synonyms or related terms only to the most informative words (rare, domain-specific) in parentheses\n",
      "- Keep it short and useful\n",
      "\n",
      "### Example\n",
      "Input: danger screen sleep\n",
      "Output: What are the risks (hazards, dangers) of using screens (monitors, displays) before sleep (slumber, bedtime)?\n",
      "\n",
      "Now rewrite this query:\n",
      "Input: how do you address a university chancellor\n",
      "\n",
      "Output: What is the first thing you should address a university chancellor?\n",
      "--------------------------------------------------\n",
      "CPU times: user 8min 44s, sys: 4.86 s, total: 8min 49s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for q in sample_queries_list:\n",
    "    print(q)\n",
    "    print(FLAN_PROMPT_TEMPLATE.format(query=q))\n",
    "    print(rewrite_with_flan(q))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "What is the legal framework (regulations, laws) surrounding prostitution in Hamilton, Ontario, Canada?\n",
      "--------------------------------------------------\n",
      "What is the objective (goal, aim) of the 2015 report compiled by the Pakistan Federal Union of Journalists (PFUJ)?\n",
      "CPU times: user 28.2 ms, sys: 12.4 ms, total: 40.6 ms\n",
      "Wall time: 703 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for q in sample_queries_list[18:20]:\n",
    "    print(\"-\"*50)\n",
    "    print(rewrite_query_llm(q,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52823, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests on refined_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_queries_df = pd.read_csv(\"refined_queries.csv\")\n",
    "df = pd.read_csv(\"raw_queries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"query\"] = df[\"query\"].str.strip()\n",
    "refined_queries_df['query'] = refined_queries_df['query'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_queries_df = pd.merge(refined_queries_df, df, on=\"query\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests on paired_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_queries_df = pd.read_csv(\"paired_queries.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9973, 4)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_queries_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_queries_df_train, paired_queries_df_test = paired_queries_df.loc[:9800, :], paired_queries_df.loc[9800:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_queries_df_train.to_csv(\"paired_queries_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_queries_df_test.to_csv(\"paired_queries_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.73.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.12/site-packages (from openai) (4.13.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.25.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.5-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from wandb) (75.8.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.73.0-py3-none-any.whl (644 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.4/644.4 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.19.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.25.1-py2.py3-none-any.whl (339 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading setproctitle-1.3.5-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-inspection, threadpoolctl, sniffio, smmap, setproctitle, sentry-sdk, scipy, safetensors, regex, pyyaml, pydantic-core, protobuf, joblib, jiter, h11, fsspec, docker-pycreds, dill, click, annotated-types, scikit-learn, pydantic, pandas, multiprocess, huggingface-hub, httpcore, gitdb, anyio, tokenizers, httpx, gitpython, wandb, transformers, openai, datasets, accelerate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2025.3.2 requires fsspec==2025.3.2.*, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed accelerate-1.6.0 annotated-types-0.7.0 anyio-4.9.0 click-8.1.8 datasets-3.5.0 dill-0.3.8 docker-pycreds-0.4.0 fsspec-2024.12.0 gitdb-4.0.12 gitpython-3.1.44 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 huggingface-hub-0.30.2 jiter-0.9.0 joblib-1.4.2 multiprocess-0.70.16 openai-1.73.0 pandas-2.2.3 protobuf-5.29.4 pydantic-2.11.3 pydantic-core-2.33.1 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentry-sdk-2.25.1 setproctitle-1.3.5 smmap-5.0.2 sniffio-1.3.1 threadpoolctl-3.6.0 tokenizers-0.21.1 transformers-4.51.2 typing-inspection-0.4.0 tzdata-2025.2 wandb-0.19.9 xxhash-3.5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'install', 'transformers', 'scikit-learn', 'openai', 'accelerate', 'pandas', 'wandb', 'datasets'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"transformers\", \"scikit-learn\", \"openai\", \"accelerate\", \"pandas\", \"wandb\", \"datasets\", \"peft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: No netrc file found, creating one.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /home/onyxia/.netrc\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['wandb', 'login', 'd1beea7e0d2ac446ba4460daa9aff7ddfc3fe41c'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"wandb\", \"login\", \"d1beea7e0d2ac446ba4460daa9aff7ddfc3fe41c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: vincent-gimenes to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.9\n",
      "wandb: Run data is saved locally in /home/onyxia/work/NLP_ENSAE/wandb/run-20250413_150603-zt1xkc7b\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run flan-t5-large-5epochs-lora\n",
      "wandb: ⭐️ View project at https://wandb.ai/NLP_ENSAE/finetunning%20T5\n",
      "wandb: 🚀 View run at https://wandb.ai/NLP_ENSAE/finetunning%20T5/runs/zt1xkc7b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,437,184 || all params: 792,587,264 || trainable%: 1.1907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9310/9310 [00:05<00:00, 1575.93 examples/s]\n",
      "Map: 100%|██████████| 491/491 [00:00<00:00, 1489.14 examples/s]\n",
      "/home/onyxia/work/NLP_ENSAE/finetunning_T5_LORA.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "  0%|          | 0/2910 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "  2%|▏         | 50/2910 [00:41<41:31,  1.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.609, 'grad_norm': 0.5104425549507141, 'learning_rate': 8.90909090909091e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 100/2910 [01:24<38:06,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9643, 'grad_norm': 0.499752014875412, 'learning_rate': 0.00017999999999999998, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 150/2910 [02:07<38:37,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5881, 'grad_norm': 0.498221755027771, 'learning_rate': 0.0002709090909090909, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 200/2910 [02:49<36:48,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.507, 'grad_norm': 0.4470294117927551, 'learning_rate': 0.00036181818181818185, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:05,  5.43it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:08,  3.47it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:09,  2.98it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.64it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:10,  2.41it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:10,  2.29it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:03<00:10,  2.24it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:09,  2.32it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.38it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:08,  2.38it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.38it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:05<00:07,  2.46it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.59it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:06,  2.62it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:06<00:05,  2.60it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.53it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:07<00:05,  2.53it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.41it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.37it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:08<00:04,  2.45it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.48it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:09<00:03,  2.48it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:09<00:02,  2.51it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.58it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:10<00:01,  2.53it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:10<00:01,  2.52it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:11<00:01,  2.65it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:11<00:00,  2.34it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:12<00:00,  2.05it/s]\u001b[A\n",
      "                                                  \n",
      "  7%|▋         | 200/2910 [03:01<36:48,  1.23it/s]\n",
      "100%|██████████| 31/31 [00:12<00:00,  2.18it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2570911645889282, 'eval_runtime': 12.9171, 'eval_samples_per_second': 38.012, 'eval_steps_per_second': 2.4, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 250/2910 [03:46<37:49,  1.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4364, 'grad_norm': 0.47990113496780396, 'learning_rate': 0.0004527272727272727, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 300/2910 [04:27<34:06,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3992, 'grad_norm': 0.49421951174736023, 'learning_rate': 0.0004954459203036053, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 350/2910 [05:09<38:26,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4047, 'grad_norm': 0.43777430057525635, 'learning_rate': 0.0004859582542694497, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 400/2910 [05:49<35:02,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3928, 'grad_norm': 0.4408038854598999, 'learning_rate': 0.0004764705882352941, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:05,  5.72it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.55it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.15it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.80it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.52it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.43it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.34it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:09,  2.44it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.48it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.50it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.51it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.57it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.73it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.76it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.73it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.68it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.66it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.54it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.50it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.57it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.63it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:03,  2.62it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:09<00:02,  2.64it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.72it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.65it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:10<00:01,  2.65it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.78it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.45it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.15it/s]\u001b[A\n",
      "                                                  \n",
      " 14%|█▎        | 400/2910 [06:01<35:02,  1.19it/s]\n",
      "100%|██████████| 31/31 [00:12<00:00,  2.29it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1770257949829102, 'eval_runtime': 12.2837, 'eval_samples_per_second': 39.972, 'eval_steps_per_second': 2.524, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 450/2910 [06:42<33:46,  1.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3506, 'grad_norm': 0.38255539536476135, 'learning_rate': 0.00046698292220113854, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 500/2910 [07:22<33:31,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3907, 'grad_norm': 0.4730367362499237, 'learning_rate': 0.00045749525616698295, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 550/2910 [08:03<30:23,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3825, 'grad_norm': 0.4399096369743347, 'learning_rate': 0.0004480075901328273, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 600/2910 [08:44<29:48,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3531, 'grad_norm': 0.47196048498153687, 'learning_rate': 0.0004385199240986718, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  6.01it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.66it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.16it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.84it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.55it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.46it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.36it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.49it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.54it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.53it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.56it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.63it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.76it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.80it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.76it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.70it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.69it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.57it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.53it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.60it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.65it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.68it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.76it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.70it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:10<00:01,  2.68it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.84it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.49it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.18it/s]\u001b[A\n",
      "                                                  \n",
      " 21%|██        | 600/2910 [08:56<29:48,  1.29it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.33it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1295217275619507, 'eval_runtime': 12.1049, 'eval_samples_per_second': 40.562, 'eval_steps_per_second': 2.561, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 650/2910 [09:37<31:48,  1.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3151, 'grad_norm': 0.5143242478370667, 'learning_rate': 0.00042903225806451614, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 700/2910 [10:18<28:13,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3026, 'grad_norm': 0.4844040274620056, 'learning_rate': 0.00041954459203036055, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 750/2910 [10:58<32:51,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2902, 'grad_norm': 0.44101056456565857, 'learning_rate': 0.0004100569259962049, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 800/2910 [11:41<29:27,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2813, 'grad_norm': 0.4443496763706207, 'learning_rate': 0.00040056925996204937, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  6.05it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.74it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.19it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.88it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.47it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.39it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.49it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.57it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.54it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.55it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.63it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.76it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.81it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.77it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.71it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.57it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.53it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.61it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.65it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:02,  2.67it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.69it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.76it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.70it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.69it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.82it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.50it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.19it/s]\u001b[A\n",
      "                                                  \n",
      " 27%|██▋       | 800/2910 [11:53<29:27,  1.19it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.34it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1077094078063965, 'eval_runtime': 12.0687, 'eval_samples_per_second': 40.684, 'eval_steps_per_second': 2.569, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 850/2910 [12:34<26:06,  1.31it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2845, 'grad_norm': 0.4517531394958496, 'learning_rate': 0.00039108159392789373, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 900/2910 [13:15<27:57,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2572, 'grad_norm': 0.42539721727371216, 'learning_rate': 0.00038159392789373814, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 950/2910 [13:55<25:40,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2861, 'grad_norm': 0.4857034683227539, 'learning_rate': 0.00037210626185958255, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1000/2910 [14:35<25:30,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3088, 'grad_norm': 0.48656049370765686, 'learning_rate': 0.00036261859582542696, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  5.88it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.71it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.21it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:08,  2.89it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.48it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.39it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.49it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.56it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.54it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.56it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.64it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.77it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.82it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.78it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.71it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:06<00:04,  2.59it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.55it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.62it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.65it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.69it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.76it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.70it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.69it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.83it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.50it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.19it/s]\u001b[A\n",
      "                                                   \n",
      " 34%|███▍      | 1000/2910 [14:47<25:30,  1.25it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.34it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.09286367893219, 'eval_runtime': 12.0665, 'eval_samples_per_second': 40.691, 'eval_steps_per_second': 2.569, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1050/2910 [15:27<25:31,  1.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2552, 'grad_norm': 0.469297856092453, 'learning_rate': 0.0003531309297912714, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1100/2910 [16:08<24:10,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2657, 'grad_norm': 0.560320258140564, 'learning_rate': 0.00034364326375711573, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 1150/2910 [16:48<23:19,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2684, 'grad_norm': 0.4840362071990967, 'learning_rate': 0.00033415559772296015, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 1200/2910 [17:30<23:02,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2311, 'grad_norm': 0.5324177145957947, 'learning_rate': 0.00032466793168880456, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  6.04it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.68it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.21it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:08,  2.89it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.48it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.39it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.49it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.56it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.55it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.57it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.63it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.78it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.82it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.77it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.72it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:06<00:04,  2.58it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.55it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.63it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.67it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:02,  2.68it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.70it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.78it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.71it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.69it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.84it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.50it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.19it/s]\u001b[A\n",
      "                                                   \n",
      " 41%|████      | 1200/2910 [17:42<23:02,  1.24it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.34it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0828557014465332, 'eval_runtime': 12.0301, 'eval_samples_per_second': 40.814, 'eval_steps_per_second': 2.577, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1250/2910 [18:22<22:31,  1.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2166, 'grad_norm': 0.5371730327606201, 'learning_rate': 0.00031518026565464897, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 1300/2910 [19:03<20:58,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1739, 'grad_norm': 0.5223187208175659, 'learning_rate': 0.0003056925996204933, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 1350/2910 [19:43<22:47,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1961, 'grad_norm': 0.47458788752555847, 'learning_rate': 0.0002962049335863378, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1400/2910 [20:25<21:30,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2051, 'grad_norm': 0.5485581755638123, 'learning_rate': 0.0002867172675521822, 'epoch': 2.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  5.88it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.65it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.19it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.84it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.46it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.39it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.48it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.54it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.53it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.56it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.63it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.76it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.80it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.76it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.70it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.70it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.57it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.54it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.61it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.65it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.67it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.76it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.70it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.69it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.84it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.50it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.18it/s]\u001b[A\n",
      "                                                   \n",
      " 48%|████▊     | 1400/2910 [20:37<21:30,  1.17it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.32it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0731769800186157, 'eval_runtime': 12.0885, 'eval_samples_per_second': 40.617, 'eval_steps_per_second': 2.564, 'epoch': 2.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 1450/2910 [21:17<18:55,  1.29it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.213, 'grad_norm': 0.48790696263313293, 'learning_rate': 0.00027722960151802656, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1500/2910 [21:57<18:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1828, 'grad_norm': 0.6680963039398193, 'learning_rate': 0.000267741935483871, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1550/2910 [22:36<18:05,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1884, 'grad_norm': 0.5297406911849976, 'learning_rate': 0.0002582542694497154, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1599/2910 [23:17<16:33,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1736, 'grad_norm': 0.5524739027023315, 'learning_rate': 0.0002487666034155598, 'epoch': 2.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1600/2910 [23:18<16:18,  1.34it/s]\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  6.06it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.66it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.19it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.87it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.48it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.39it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.49it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.55it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.54it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.57it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.64it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.77it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.82it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.77it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.71it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.57it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.53it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.60it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.65it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.68it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.76it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.69it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.68it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.83it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.50it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.19it/s]\u001b[A\n",
      "                                                   \n",
      " 55%|█████▍    | 1600/2910 [23:30<16:18,  1.34it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.34it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0647612810134888, 'eval_runtime': 12.0637, 'eval_samples_per_second': 40.701, 'eval_steps_per_second': 2.57, 'epoch': 2.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1650/2910 [24:11<16:59,  1.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2257, 'grad_norm': 0.513951301574707, 'learning_rate': 0.00023927893738140418, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1700/2910 [24:51<15:35,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1878, 'grad_norm': 0.5920600295066833, 'learning_rate': 0.0002297912713472486, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1750/2910 [25:33<17:51,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1835, 'grad_norm': 0.5080300569534302, 'learning_rate': 0.00022030360531309298, 'epoch': 3.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1800/2910 [26:14<16:32,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1318, 'grad_norm': 0.4912083148956299, 'learning_rate': 0.0002108159392789374, 'epoch': 3.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  6.03it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.66it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.89it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.48it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.39it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.49it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.56it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.55it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.57it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.63it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.78it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.82it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.76it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.71it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:06<00:04,  2.57it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.53it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.61it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:02,  2.67it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.70it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.78it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.71it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.69it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.84it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.49it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.19it/s]\u001b[A\n",
      "                                                   \n",
      " 62%|██████▏   | 1800/2910 [26:26<16:32,  1.12it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.33it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0607737302780151, 'eval_runtime': 12.0461, 'eval_samples_per_second': 40.76, 'eval_steps_per_second': 2.573, 'epoch': 3.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 1850/2910 [27:07<14:05,  1.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1263, 'grad_norm': 0.5331314206123352, 'learning_rate': 0.00020132827324478178, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1900/2910 [27:47<14:19,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1346, 'grad_norm': 0.6531055569648743, 'learning_rate': 0.0001918406072106262, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1950/2910 [28:27<13:05,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1643, 'grad_norm': 0.6841012835502625, 'learning_rate': 0.00018235294117647057, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 2000/2910 [29:09<12:08,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1536, 'grad_norm': 0.5695179104804993, 'learning_rate': 0.00017286527514231498, 'epoch': 3.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  5.90it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.69it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.87it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.47it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.39it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.49it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.55it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.54it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.57it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.65it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.78it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.83it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.78it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.71it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:06<00:04,  2.58it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.54it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.62it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.69it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.76it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.70it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.69it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.82it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.49it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.18it/s]\u001b[A\n",
      "                                                   \n",
      " 69%|██████▊   | 2000/2910 [29:21<12:08,  1.25it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.32it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0552116632461548, 'eval_runtime': 12.0608, 'eval_samples_per_second': 40.711, 'eval_steps_per_second': 2.57, 'epoch': 3.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2049/2910 [29:59<10:36,  1.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1173, 'grad_norm': 0.5346043109893799, 'learning_rate': 0.0001633776091081594, 'epoch': 3.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 2100/2910 [30:40<10:21,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1472, 'grad_norm': 0.5332534909248352, 'learning_rate': 0.0001538899430740038, 'epoch': 3.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 2150/2910 [31:19<08:47,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.117, 'grad_norm': 0.5129985213279724, 'learning_rate': 0.00014440227703984822, 'epoch': 3.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 2200/2910 [32:01<10:06,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1132, 'grad_norm': 0.6393784880638123, 'learning_rate': 0.0001349146110056926, 'epoch': 3.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  6.00it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.72it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.19it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.88it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.47it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.38it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.49it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.55it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.53it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.56it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.63it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.76it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.81it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.76it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.70it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.71it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.56it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.54it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.61it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.64it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.66it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.76it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.70it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.68it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.85it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.50it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.18it/s]\u001b[A\n",
      "                                                   \n",
      " 76%|███████▌  | 2200/2910 [32:13<10:06,  1.17it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.32it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0464096069335938, 'eval_runtime': 12.0951, 'eval_samples_per_second': 40.595, 'eval_steps_per_second': 2.563, 'epoch': 3.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 2250/2910 [32:52<09:12,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1337, 'grad_norm': 0.5809209942817688, 'learning_rate': 0.00012542694497153702, 'epoch': 3.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 2300/2910 [33:33<07:57,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1378, 'grad_norm': 0.5999079942703247, 'learning_rate': 0.0001159392789373814, 'epoch': 3.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 2350/2910 [34:15<06:55,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0998, 'grad_norm': 0.5572148561477661, 'learning_rate': 0.0001064516129032258, 'epoch': 4.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 2400/2910 [34:55<06:40,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0893, 'grad_norm': 0.5167026519775391, 'learning_rate': 9.696394686907021e-05, 'epoch': 4.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  5.83it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.69it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.21it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.85it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.46it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.38it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.49it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.54it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.53it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.56it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.63it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.76it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.81it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.77it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.71it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.59it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.54it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.61it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.68it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.77it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.70it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.68it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.84it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.49it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.18it/s]\u001b[A\n",
      "                                                   \n",
      " 82%|████████▏ | 2400/2910 [35:07<06:40,  1.27it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.32it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0457242727279663, 'eval_runtime': 12.077, 'eval_samples_per_second': 40.656, 'eval_steps_per_second': 2.567, 'epoch': 4.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 2450/2910 [35:48<06:44,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.09, 'grad_norm': 0.6209102869033813, 'learning_rate': 8.747628083491461e-05, 'epoch': 4.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 2499/2910 [36:28<05:46,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1144, 'grad_norm': 0.551062822341919, 'learning_rate': 7.798861480075901e-05, 'epoch': 4.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 2550/2910 [37:09<04:38,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0815, 'grad_norm': 0.5454602241516113, 'learning_rate': 6.850094876660342e-05, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 2600/2910 [37:50<03:59,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0757, 'grad_norm': 0.5983353853225708, 'learning_rate': 5.901328273244782e-05, 'epoch': 4.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  5.97it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.72it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.19it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.86it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.47it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.38it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.48it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.54it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.53it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.55it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.62it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.76it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.81it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.76it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.71it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.57it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.53it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.60it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:02,  2.67it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.68it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.76it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.71it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.69it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.83it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.49it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.18it/s]\u001b[A\n",
      "                                                   \n",
      " 89%|████████▉ | 2600/2910 [38:02<03:59,  1.29it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.32it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0409901142120361, 'eval_runtime': 12.1021, 'eval_samples_per_second': 40.572, 'eval_steps_per_second': 2.562, 'epoch': 4.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 2650/2910 [38:43<03:45,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0849, 'grad_norm': 0.6565913558006287, 'learning_rate': 4.9525616698292216e-05, 'epoch': 4.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2700/2910 [39:23<02:56,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0906, 'grad_norm': 0.5816260576248169, 'learning_rate': 4.003795066413662e-05, 'epoch': 4.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 2750/2910 [40:03<01:59,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0875, 'grad_norm': 0.5706031322479248, 'learning_rate': 3.0550284629981026e-05, 'epoch': 4.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 2800/2910 [40:43<01:27,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1035, 'grad_norm': 0.6130892634391785, 'learning_rate': 2.1062618595825425e-05, 'epoch': 4.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 2/31 [00:00<00:04,  5.83it/s]\u001b[A\n",
      " 10%|▉         | 3/31 [00:00<00:07,  3.65it/s]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:01<00:08,  3.18it/s]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:01<00:09,  2.84it/s]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:02<00:09,  2.56it/s]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:02<00:09,  2.46it/s]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:02<00:09,  2.38it/s]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:03<00:08,  2.48it/s]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:03<00:08,  2.54it/s]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:04<00:07,  2.54it/s]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:04<00:07,  2.55it/s]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:04<00:06,  2.62it/s]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:05<00:06,  2.77it/s]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:05<00:05,  2.80it/s]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:05<00:05,  2.76it/s]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:06<00:05,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:06<00:04,  2.69it/s]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:07<00:04,  2.57it/s]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:07<00:04,  2.54it/s]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:07<00:03,  2.61it/s]\u001b[A\n",
      " 71%|███████   | 22/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:08<00:03,  2.66it/s]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:08<00:02,  2.67it/s]\u001b[A\n",
      " 81%|████████  | 25/31 [00:09<00:02,  2.77it/s]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:09<00:01,  2.70it/s]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:09<00:01,  2.69it/s]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:10<00:01,  2.84it/s]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:10<00:00,  2.49it/s]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:11<00:00,  2.18it/s]\u001b[A\n",
      "                                                   \n",
      " 96%|█████████▌| 2800/2910 [40:55<01:27,  1.26it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.32it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0410146713256836, 'eval_runtime': 12.0991, 'eval_samples_per_second': 40.581, 'eval_steps_per_second': 2.562, 'epoch': 4.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 2850/2910 [41:36<00:49,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0719, 'grad_norm': 0.6414581537246704, 'learning_rate': 1.157495256166983e-05, 'epoch': 4.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2900/2910 [42:17<00:08,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1031, 'grad_norm': 0.686771810054779, 'learning_rate': 2.0872865275142314e-06, 'epoch': 4.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2910/2910 [42:28<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2548.2639, 'train_samples_per_second': 18.267, 'train_steps_per_second': 1.142, 'train_loss': 1.2553657659550304, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:               eval/loss █▅▄▃▃▂▂▂▂▁▁▁▁▁\n",
      "wandb:            eval/runtime █▃▂▁▁▁▁▁▁▁▂▁▂▂\n",
      "wandb: eval/samples_per_second ▁▆▇███████▇█▇▇\n",
      "wandb:   eval/steps_per_second ▁▆▇███▇███▇█▇▇\n",
      "wandb:             train/epoch ▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██\n",
      "wandb:       train/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "wandb:         train/grad_norm ▃▃▃▂▂▁▂▃▁▂▁▃▃▂▅▄▄▄▂▄▇▄▄▃▅▇█▄▄▃▆▅▃▆▄▆▇▅▆█\n",
      "wandb:     train/learning_rate ▂▄▆▇███▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "wandb:              train/loss █▃▃▃▂▂▂▂▂▂▂▂���▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                eval/loss 1.04101\n",
      "wandb:             eval/runtime 12.0991\n",
      "wandb:  eval/samples_per_second 40.581\n",
      "wandb:    eval/steps_per_second 2.562\n",
      "wandb:               total_flos 6832462296563712.0\n",
      "wandb:              train/epoch 5\n",
      "wandb:        train/global_step 2910\n",
      "wandb:          train/grad_norm 0.68677\n",
      "wandb:      train/learning_rate 0.0\n",
      "wandb:               train/loss 1.1031\n",
      "wandb:               train_loss 1.25537\n",
      "wandb:            train_runtime 2548.2639\n",
      "wandb: train_samples_per_second 18.267\n",
      "wandb:   train_steps_per_second 1.142\n",
      "wandb: \n",
      "wandb: 🚀 View run flan-t5-large-5epochs-lora at: https://wandb.ai/NLP_ENSAE/finetunning%20T5/runs/zt1xkc7b\n",
      "wandb: ⭐️ View project at: https://wandb.ai/NLP_ENSAE/finetunning%20T5\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20250413_150603-zt1xkc7b/logs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'finetunning_T5_LORA.py'], returncode=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"python\", \"finetunning_T5_LORA.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('data_folder/paired_queries_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = df_test[\"noisy_query\"].values[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# Chargement du modèle fine-tuné\n",
    "model_path = \"./flan-t5-small-rewriting\"  # ou base/large selon ce que tu as entraîné\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Input query : ho vouces dale on kinf of the hill\n",
      "✍️ Rewritten   : What is the significance (importance, significance) of the dale on the kinf of the hill (sea, sand)?\n",
      "🔎 Input query : ha features ewre availalt on B-R for th e0211 ppeing Dauz\n",
      "✍️ Rewritten   : What features (features, features) are available on B-R for the e0211 season (season, season)?\n",
      "🔎 Input query : wha is the color of horseshoe crab blood\n",
      "✍️ Rewritten   : What is the color (color, color) of horseshoe crab blood (blood, blood)?\n",
      "🔎 Input query : did king david ever existed\n",
      "✍️ Rewritten   : Did King David ever exist (exist, exist) in the United States (USA, America)?\n",
      "🔎 Input query : What is the primary purpose of any corporation according to economist Roger Martin?\n",
      "✍️ Rewritten   : What is the primary objective (goal, objective) of any corporation according to economist Roger Martin?\n",
      "🔎 Input query : what is teacjing english as a second language\n",
      "✍️ Rewritten   : What is the primary language (language, language) of English (English, English)?\n",
      "🔎 Input query : whsscycloporuasos\n",
      "✍️ Rewritten   : What are the characteristics (characteristics, characteristics) of cycloporuasos (cycloporuasos, cycloporuasos)?\n",
      "🔎 Input query : who played jason on friday the 13th part 2\n",
      "✍️ Rewritten   : Who played the role of Jason on the TV show Friday the 13th Part 2?\n",
      "🔎 Input query : What are some of the best cricke betting apps in India according to the context\n",
      "✍️ Rewritten   : What are the top cricke betting apps (apps, betting apps) in India according to the given context?\n",
      "🔎 Input query : What is the address of Dilek Gokcin Coskun?\n",
      "✍️ Rewritten   : What is the address (postal address, address) of Dilek Gokcin Coskun?\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    input_text = \"Improve the query: \" + query\n",
    "\n",
    "    # Tokenisation\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Génération\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    rewritten = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"🔎 Input query :\", query)\n",
    "    print(\"✍️ Rewritten   :\", rewritten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# Chargement du modèle fine-tuné\n",
    "model_path = \"./flan-t5-base-rewriting\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Input query : ho vouces dale on kinf of the hill\n",
      "✍️ Rewritten   : Who plays the role of Dale on the TV show \"The Kinf of the Hill\"?\n",
      "🔎 Input query : ha features ewre availalt on B-R for th e0211 ppeing Dauz\n",
      "✍️ Rewritten   : What features are available on B-R for the 0211 season of the TV show \"The Dauz\"?\n",
      "🔎 Input query : wha is the color of horseshoe crab blood\n",
      "✍️ Rewritten   : What is the color (color, hue) of horseshoe crab blood (blood, blood)?\n",
      "🔎 Input query : did king david ever existed\n",
      "✍️ Rewritten   : Did King David ever exist (exist, exist)?\n",
      "🔎 Input query : What is the primary purpose of any corporation according to economist Roger Martin?\n",
      "✍️ Rewritten   : What is the primary objective (goal, objective) of any corporation according to economist Roger Martin's perspective (viewpoint, perspective)?\n",
      "🔎 Input query : what is teacjing english as a second language\n",
      "✍️ Rewritten   : What is the significance (importance, significance) of English as a second language (language, dialect)?\n",
      "🔎 Input query : whsscycloporuasos\n",
      "✍️ Rewritten   : What is cycloporus (synthetic cyclosporin, cyclosporin)?\n",
      "🔎 Input query : who played jason on friday the 13th part 2\n",
      "✍️ Rewritten   : Who portrayed (played, acted) Jason on the TV show Friday the 13th Part 2?\n",
      "🔎 Input query : What are some of the best cricke betting apps in India according to the context\n",
      "✍️ Rewritten   : What are some of the best cricket betting apps (software, services) in India according to the given context (situation, circumstances)?\n",
      "🔎 Input query : What is the address of Dilek Gokcin Coskun?\n",
      "✍️ Rewritten   : What is the address (name, phone number) of Dilek Gokcin Coskun?\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    input_text = \"Improve the query: \" + query\n",
    "\n",
    "    # Tokenisation\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Génération\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    rewritten = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"🔎 Input query :\", query)\n",
    "    print(\"✍️ Rewritten   :\", rewritten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Large (lora 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# Chargement du modèle fine-tuné\n",
    "model_path = \"./flan-t5-large-rewriting-lora\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Input query : ho vouces dale on kinf of the hill\n",
      "✍️ Rewritten   : What is the name of the valley (dale, valley) on the summit of Mount Kilimanjaro?\n",
      "🔎 Input query : ha features ewre availalt on B-R for th e0211 ppeing Dauz\n",
      "✍️ Rewritten   : What are the features (features, amenities) available on B-R for the E0211 eing Dauz?\n",
      "🔎 Input query : wha is the color of horseshoe crab blood\n",
      "✍️ Rewritten   : What is the color (tone, hue) of horseshoe crab blood (blood, blood)?\n",
      "🔎 Input query : did king david ever existed\n",
      "✍️ Rewritten   : Did King David (the King of Israel) ever exist (exist, exist)?\n",
      "🔎 Input query : What is the primary purpose of any corporation according to economist Roger Martin?\n",
      "✍️ Rewritten   : What is the primary objective (purpose, goal) of any corporation (organization, business) according to economist Roger Martin?\n",
      "🔎 Input query : what is teacjing english as a second language\n",
      "✍️ Rewritten   : What is the definition (meaning, meaning) of teaching English as a second language (second language, second language)?\n",
      "🔎 Input query : whsscycloporuasos\n",
      "✍️ Rewritten   : What is the Cycloporus (cycloporus, cyclopod)?\n",
      "🔎 Input query : who played jason on friday the 13th part 2\n",
      "✍️ Rewritten   : Who played the role of Jason (Jason, Jason) on Friday the 13th Part 2?\n",
      "🔎 Input query : What are some of the best cricke betting apps in India according to the context\n",
      "✍️ Rewritten   : What are some of the best cricket betting apps (apps, websites) in India, based on the context (contextual context, context)?\n",
      "🔎 Input query : What is the address of Dilek Gokcin Coskun?\n",
      "✍️ Rewritten   : What is the address (location, location) of Dilek Gokcin Coskun's home?\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    input_text = \"Improve the query: \" + query\n",
    "\n",
    "    # Tokenisation\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Génération\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    rewritten = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"🔎 Input query :\", query)\n",
    "    print(\"✍️ Rewritten   :\", rewritten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Large (lora 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# Chargement du modèle fine-tuné\n",
    "model_path = \"./flan-t5-large-rewriting-lora-2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Input query : ho vouces dale on kinf of the hill\n",
      "✍️ Rewritten   : Who plays the role of Dale (character, character) on the TV show \"Kinf of the Hill\"?\n",
      "🔎 Input query : ha features ewre availalt on B-R for th e0211 ppeing Dauz\n",
      "✍️ Rewritten   : What features are available on B-River (B-River Radio) for the February 11th episode of the TV show Dauz?\n",
      "🔎 Input query : wha is the color of horseshoe crab blood\n",
      "✍️ Rewritten   : What is the color of horseshoe crab blood (blood, fluid)?\n",
      "🔎 Input query : did king david ever existed\n",
      "✍️ Rewritten   : Did King David ever exist (exist, exist) in history (history, history)?\n",
      "🔎 Input query : What is the primary purpose of any corporation according to economist Roger Martin?\n",
      "✍️ Rewritten   : What is the primary objective (goal, aim) of any business (corporation, firm) according to economist Roger Martin's perspective (viewpoint, perspective)?\n",
      "🔎 Input query : what is teacjing english as a second language\n",
      "✍️ Rewritten   : What is the process (method, technique) of learning English as a second language (second language, second language learning)?\n",
      "🔎 Input query : whsscycloporuasos\n",
      "✍️ Rewritten   : What are cyclopropane (CPO) batteries (batteries, batteries)?\n",
      "🔎 Input query : who played jason on friday the 13th part 2\n",
      "✍️ Rewritten   : Who portrayed (played, acted) Jason in Friday the 13th Part 2?\n",
      "🔎 Input query : What are some of the best cricke betting apps in India according to the context\n",
      "✍️ Rewritten   : What are the top (best, top) cricket betting apps in India according to the given context (situation, circumstance)?\n",
      "🔎 Input query : What is the address of Dilek Gokcin Coskun?\n",
      "✍️ Rewritten   : What is the address of Dilek Gokcin Coskun's residence (home, office)?\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    input_text = \"Improve the query: \" + query\n",
    "\n",
    "    # Tokenisation\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Génération\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    rewritten = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"🔎 Input query :\", query)\n",
    "    print(\"✍️ Rewritten   :\", rewritten)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
